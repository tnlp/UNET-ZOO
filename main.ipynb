{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFbRE+V05pFFT8ci0LlMw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnlp/UNET-ZOO/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3EgTuMXL3qz"
      },
      "source": [
        "'''\n",
        "author:zhujunwen\n",
        "Guangdong University of Technology\n",
        "'''\n",
        "import argparse\n",
        "import logging\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import autograd, optim\n",
        "from UNet import Unet,resnet34_unet\n",
        "from attention_unet import AttU_Net\n",
        "from channel_unet import myChannelUnet\n",
        "from r2unet import R2U_Net\n",
        "from segnet import SegNet\n",
        "from unetpp import NestedUNet\n",
        "from fcn import get_fcn8s\n",
        "from dataset import *\n",
        "from metrics import *\n",
        "from torchvision.transforms import transforms\n",
        "from plot import loss_plot\n",
        "from plot import metrics_plot\n",
        "from torchvision.models import vgg16\n",
        "def getArgs():\n",
        "    parse = argparse.ArgumentParser()\n",
        "    parse.add_argument('--deepsupervision', default=0)\n",
        "    parse.add_argument(\"--action\", type=str, help=\"train/test/train&test\", default=\"train&test\")\n",
        "    parse.add_argument(\"--epoch\", type=int, default=21)\n",
        "    parse.add_argument('--arch', '-a', metavar='ARCH', default='resnet34_unet',\n",
        "                       help='UNet/resnet34_unet/unet++/myChannelUnet/Attention_UNet/segnet/r2unet/fcn32s/fcn8s')\n",
        "    parse.add_argument(\"--batch_size\", type=int, default=1)\n",
        "    parse.add_argument('--dataset', default='driveEye',  # dsb2018_256\n",
        "                       help='dataset name:liver/esophagus/dsb2018Cell/corneal/driveEye/isbiCell/kaggleLung')\n",
        "    # parse.add_argument(\"--ckp\", type=str, help=\"the path of model weight file\")\n",
        "    parse.add_argument(\"--log_dir\", default='result/log', help=\"log dir\")\n",
        "    parse.add_argument(\"--threshold\",type=float,default=None)\n",
        "    args = parse.parse_args()\n",
        "    return args\n",
        "\n",
        "def getLog(args):\n",
        "    dirname = os.path.join(args.log_dir,args.arch,str(args.batch_size),str(args.dataset),str(args.epoch))\n",
        "    filename = dirname +'/log.log'\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    logging.basicConfig(\n",
        "            filename=filename,\n",
        "            level=logging.DEBUG,\n",
        "            format='%(asctime)s:%(levelname)s:%(message)s'\n",
        "        )\n",
        "    return logging\n",
        "\n",
        "def getModel(args):\n",
        "    if args.arch == 'UNet':\n",
        "        model = Unet(3, 1).to(device)\n",
        "    if args.arch == 'resnet34_unet':\n",
        "        model = resnet34_unet(1,pretrained=False).to(device)\n",
        "    if args.arch == 'unet++':\n",
        "        args.deepsupervision = True\n",
        "        model = NestedUNet(args,3,1).to(device)\n",
        "    if args.arch =='Attention_UNet':\n",
        "        model = AttU_Net(3,1).to(device)\n",
        "    if args.arch == 'segnet':\n",
        "        model = SegNet(3,1).to(device)\n",
        "    if args.arch == 'r2unet':\n",
        "        model = R2U_Net(3,1).to(device)\n",
        "    # if args.arch == 'fcn32s':\n",
        "    #     model = get_fcn32s(1).to(device)\n",
        "    if args.arch == 'myChannelUnet':\n",
        "        model = myChannelUnet(3,1).to(device)\n",
        "    if args.arch == 'fcn8s':\n",
        "        assert args.dataset !='esophagus' ,\"fcn8s模型不能用于数据集esophagus，因为esophagus数据集为80x80，经过5次的2倍降采样后剩下2.5x2.5，分辨率不能为小数，建议把数据集resize成更高的分辨率再用于fcn\"\n",
        "        model = get_fcn8s(1).to(device)\n",
        "    if args.arch == 'cenet':\n",
        "        from cenet import CE_Net_\n",
        "        model = CE_Net_().to(device)\n",
        "    return model\n",
        "\n",
        "def getDataset(args):\n",
        "    train_dataloaders, val_dataloaders ,test_dataloaders= None,None,None\n",
        "    if args.dataset =='liver':  #E:\\代码\\new\\u_net_liver-master\\data\\liver\\val\n",
        "        train_dataset = LiverDataset(r\"train\", transform=x_transforms, target_transform=y_transforms)\n",
        "        train_dataloaders = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "        val_dataset = LiverDataset(r\"val\", transform=x_transforms, target_transform=y_transforms)\n",
        "        val_dataloaders = DataLoader(val_dataset, batch_size=1)\n",
        "        test_dataloaders = val_dataloaders\n",
        "    if args.dataset ==\"esophagus\":\n",
        "        train_dataset = esophagusDataset(r\"train\", transform=x_transforms,target_transform=y_transforms)\n",
        "        train_dataloaders = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "        val_dataset = esophagusDataset(r\"val\", transform=x_transforms,target_transform=y_transforms)\n",
        "        val_dataloaders = DataLoader(val_dataset, batch_size=1)\n",
        "        test_dataloaders = val_dataloaders\n",
        "    if args.dataset == \"dsb2018Cell\":\n",
        "        train_dataset = dsb2018CellDataset(r\"train\", transform=x_transforms, target_transform=y_transforms)\n",
        "        train_dataloaders = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "        val_dataset = dsb2018CellDataset(r\"val\", transform=x_transforms, target_transform=y_transforms)\n",
        "        val_dataloaders = DataLoader(val_dataset, batch_size=1)\n",
        "        test_dataloaders = val_dataloaders\n",
        "    if args.dataset == 'corneal':\n",
        "        train_dataset = CornealDataset(r'train',transform=x_transforms, target_transform=y_transforms)\n",
        "        train_dataloaders = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "        val_dataset = CornealDataset(r\"val\", transform=x_transforms, target_transform=y_transforms)\n",
        "        val_dataloaders = DataLoader(val_dataset, batch_size=1)\n",
        "        test_dataset = CornealDataset(r\"test\", transform=x_transforms, target_transform=y_transforms)\n",
        "        test_dataloaders = DataLoader(test_dataset, batch_size=1)\n",
        "    if args.dataset == 'driveEye':\n",
        "        train_dataset = DriveEyeDataset(r'train', transform=x_transforms, target_transform=y_transforms)\n",
        "        train_dataloaders = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "        val_dataset = DriveEyeDataset(r\"val\", transform=x_transforms, target_transform=y_transforms)\n",
        "        val_dataloaders = DataLoader(val_dataset, batch_size=1)\n",
        "        test_dataset = DriveEyeDataset(r\"test\", transform=x_transforms, target_transform=y_transforms)\n",
        "        test_dataloaders = DataLoader(test_dataset, batch_size=1)\n",
        "    if args.dataset == 'isbiCell':\n",
        "        train_dataset = IsbiCellDataset(r'train', transform=x_transforms, target_transform=y_transforms)\n",
        "        train_dataloaders = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "        val_dataset = IsbiCellDataset(r\"val\", transform=x_transforms, target_transform=y_transforms)\n",
        "        val_dataloaders = DataLoader(val_dataset, batch_size=1)\n",
        "        test_dataset = IsbiCellDataset(r\"test\", transform=x_transforms, target_transform=y_transforms)\n",
        "        test_dataloaders = DataLoader(test_dataset, batch_size=1)\n",
        "    if args.dataset == 'kaggleLung':\n",
        "        train_dataset = LungKaggleDataset(r'train', transform=x_transforms, target_transform=y_transforms)\n",
        "        train_dataloaders = DataLoader(train_dataset, batch_size=args.batch_size)\n",
        "        val_dataset = LungKaggleDataset(r\"val\", transform=x_transforms, target_transform=y_transforms)\n",
        "        val_dataloaders = DataLoader(val_dataset, batch_size=1)\n",
        "        test_dataset = LungKaggleDataset(r\"test\", transform=x_transforms, target_transform=y_transforms)\n",
        "        test_dataloaders = DataLoader(test_dataset, batch_size=1)\n",
        "    return train_dataloaders,val_dataloaders,test_dataloaders\n",
        "\n",
        "def val(model,best_iou,val_dataloaders):\n",
        "    model= model.eval()\n",
        "    with torch.no_grad():\n",
        "        i=0   #验证集中第i张图\n",
        "        miou_total = 0\n",
        "        hd_total = 0\n",
        "        dice_total = 0\n",
        "        num = len(val_dataloaders)  #验证集图片的总数\n",
        "        #print(num)\n",
        "        for x, _,pic,mask in val_dataloaders:\n",
        "            x = x.to(device)\n",
        "            y = model(x)\n",
        "            if args.deepsupervision:\n",
        "                img_y = torch.squeeze(y[-1]).cpu().numpy()\n",
        "            else:\n",
        "                img_y = torch.squeeze(y).cpu().numpy()  #输入损失函数之前要把预测图变成numpy格式，且为了跟训练图对应，要额外加多一维表示batchsize\n",
        "\n",
        "            hd_total += get_hd(mask[0], img_y)\n",
        "            miou_total += get_iou(mask[0],img_y)  #获取当前预测图的miou，并加到总miou中\n",
        "            dice_total += get_dice(mask[0],img_y)\n",
        "            if i < num:i+=1   #处理验证集下一张图\n",
        "        aver_iou = miou_total / num\n",
        "        aver_hd = hd_total / num\n",
        "        aver_dice = dice_total/num\n",
        "        print('Miou=%f,aver_hd=%f,aver_dice=%f' % (aver_iou,aver_hd,aver_dice))\n",
        "        logging.info('Miou=%f,aver_hd=%f,aver_dice=%f' % (aver_iou,aver_hd,aver_dice))\n",
        "        if aver_iou > best_iou:\n",
        "            print('aver_iou:{} > best_iou:{}'.format(aver_iou,best_iou))\n",
        "            logging.info('aver_iou:{} > best_iou:{}'.format(aver_iou,best_iou))\n",
        "            logging.info('===========>save best model!')\n",
        "            best_iou = aver_iou\n",
        "            print('===========>save best model!')\n",
        "            torch.save(model.state_dict(), r'./saved_model/'+str(args.arch)+'_'+str(args.batch_size)+'_'+str(args.dataset)+'_'+str(args.epoch)+'.pth')\n",
        "        return best_iou,aver_iou,aver_dice,aver_hd\n",
        "\n",
        "def train(model, criterion, optimizer, train_dataloader,val_dataloader, args):\n",
        "    best_iou,aver_iou,aver_dice,aver_hd = 0,0,0,0\n",
        "    num_epochs = args.epoch\n",
        "    threshold = args.threshold\n",
        "    loss_list = []\n",
        "    iou_list = []\n",
        "    dice_list = []\n",
        "    hd_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model = model.train()\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        logging.info('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        dt_size = len(train_dataloader.dataset)\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for x, y,_,mask in train_dataloader:\n",
        "            step += 1\n",
        "            inputs = x.to(device)\n",
        "            labels = y.to(device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            if args.deepsupervision:\n",
        "                outputs = model(inputs)\n",
        "                loss = 0\n",
        "                for output in outputs:\n",
        "                    loss += criterion(output, labels)\n",
        "                loss /= len(outputs)\n",
        "            else:\n",
        "                output = model(inputs)\n",
        "                loss = criterion(output, labels)\n",
        "            if threshold!=None:\n",
        "                if loss > threshold:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += loss.item()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            print(\"%d/%d,train_loss:%0.3f\" % (step, (dt_size - 1) // train_dataloader.batch_size + 1, loss.item()))\n",
        "            logging.info(\"%d/%d,train_loss:%0.3f\" % (step, (dt_size - 1) // train_dataloader.batch_size + 1, loss.item()))\n",
        "        loss_list.append(epoch_loss)\n",
        "\n",
        "        best_iou,aver_iou,aver_dice,aver_hd = val(model,best_iou,val_dataloader)\n",
        "        iou_list.append(aver_iou)\n",
        "        dice_list.append(aver_dice)\n",
        "        hd_list.append(aver_hd)\n",
        "        print(\"epoch %d loss:%0.3f\" % (epoch, epoch_loss))\n",
        "        logging.info(\"epoch %d loss:%0.3f\" % (epoch, epoch_loss))\n",
        "    loss_plot(args, loss_list)\n",
        "    metrics_plot(args, 'iou&dice',iou_list, dice_list)\n",
        "    metrics_plot(args,'hd',hd_list)\n",
        "    return model\n",
        "\n",
        "def test(val_dataloaders,save_predict=False):\n",
        "    logging.info('final test........')\n",
        "    if save_predict ==True:\n",
        "        dir = os.path.join(r'./saved_predict',str(args.arch),str(args.batch_size),str(args.epoch),str(args.dataset))\n",
        "        if not os.path.exists(dir):\n",
        "            os.makedirs(dir)\n",
        "        else:\n",
        "            print('dir already exist!')\n",
        "    model.load_state_dict(torch.load(r'./saved_model/'+str(args.arch)+'_'+str(args.batch_size)+'_'+str(args.dataset)+'_'+str(args.epoch)+'.pth', map_location='cpu'))  # 载入训练好的模型\n",
        "    model.eval()\n",
        "\n",
        "    #plt.ion() #开启动态模式\n",
        "    with torch.no_grad():\n",
        "        i=0   #验证集中第i张图\n",
        "        miou_total = 0\n",
        "        hd_total = 0\n",
        "        dice_total = 0\n",
        "        num = len(val_dataloaders)  #验证集图片的总数\n",
        "        for pic,_,pic_path,mask_path in val_dataloaders:\n",
        "            pic = pic.to(device)\n",
        "            predict = model(pic)\n",
        "            if args.deepsupervision:\n",
        "                predict = torch.squeeze(predict[-1]).cpu().numpy()\n",
        "            else:\n",
        "                predict = torch.squeeze(predict).cpu().numpy()  #输入损失函数之前要把预测图变成numpy格式，且为了跟训练图对应，要额外加多一维表示batchsize\n",
        "            #img_y = torch.squeeze(y).cpu().numpy()  #输入损失函数之前要把预测图变成numpy格式，且为了跟训练图对应，要额外加多一维表示batchsize\n",
        "\n",
        "            iou = get_iou(mask_path[0],predict)\n",
        "            miou_total += iou  #获取当前预测图的miou，并加到总miou中\n",
        "            hd_total += get_hd(mask_path[0], predict)\n",
        "            dice = get_dice(mask_path[0],predict)\n",
        "            dice_total += dice\n",
        "\n",
        "            fig = plt.figure()\n",
        "            ax1 = fig.add_subplot(1, 3, 1)\n",
        "            ax1.set_title('input')\n",
        "            plt.imshow(Image.open(pic_path[0]))\n",
        "            #print(pic_path[0])\n",
        "            ax2 = fig.add_subplot(1, 3, 2)\n",
        "            ax2.set_title('predict')\n",
        "            plt.imshow(predict,cmap='Greys_r')\n",
        "            ax3 = fig.add_subplot(1, 3, 3)\n",
        "            ax3.set_title('mask')\n",
        "            plt.imshow(Image.open(mask_path[0]), cmap='Greys_r')\n",
        "            #print(mask_path[0])\n",
        "            if save_predict == True:\n",
        "                if args.dataset == 'driveEye':\n",
        "                    saved_predict = dir + '/' + mask_path[0].split('\\\\')[-1]\n",
        "                    saved_predict = '.'+saved_predict.split('.')[1] + '.tif'\n",
        "                    plt.savefig(saved_predict)\n",
        "                else:\n",
        "                    plt.savefig(dir +'/'+ mask_path[0].split('\\\\')[-1])\n",
        "            #plt.pause(0.01)\n",
        "            print('iou={},dice={}'.format(iou,dice))\n",
        "            if i < num:i+=1   #处理验证集下一张图\n",
        "        #plt.show()\n",
        "        print('Miou=%f,aver_hd=%f,dv=%f' % (miou_total/num,hd_total/num,dice_total/num))\n",
        "        logging.info('Miou=%f,aver_hd=%f,dv=%f' % (miou_total/num,hd_total/num,dice_total/num))\n",
        "        #print('M_dice=%f' % (dice_total / num))\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    x_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),  # -> [0,1]\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # ->[-1,1]\n",
        "    ])\n",
        "\n",
        "    # mask只需要转换为tensor\n",
        "    y_transforms = transforms.ToTensor()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    args = getArgs()\n",
        "    logging = getLog(args)\n",
        "    print('**************************')\n",
        "    print('models:%s,\\nepoch:%s,\\nbatch size:%s\\ndataset:%s' % \\\n",
        "          (args.arch, args.epoch, args.batch_size,args.dataset))\n",
        "    logging.info('\\n=======\\nmodels:%s,\\nepoch:%s,\\nbatch size:%s\\ndataset:%s\\n========' % \\\n",
        "          (args.arch, args.epoch, args.batch_size,args.dataset))\n",
        "    print('**************************')\n",
        "    model = getModel(args)\n",
        "    train_dataloaders,val_dataloaders,test_dataloaders = getDataset(args)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    if 'train' in args.action:\n",
        "        train(model, criterion, optimizer, train_dataloaders,val_dataloaders, args)\n",
        "    if 'test' in args.action:\n",
        "        test(test_dataloaders, save_predict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGdbps5gMdZZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}